#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Jun 27 08:41:02 2020

@author: weinie
"""

from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.converter import TextConverter
from pdfminer.layout import LAParams
from pdfminer.pdfpage import PDFPage
from io import BytesIO

def pdf_to_text(path):
    manager = PDFResourceManager()
    retstr = BytesIO()
    layout = LAParams(all_texts=True)
    device = TextConverter(manager, retstr, laparams=layout)
    filepath = open(path, 'rb')
    interpreter = PDFPageInterpreter(manager, device)

    for page in PDFPage.get_pages(filepath, check_extractable=True):
        interpreter.process_page(page)

    text = retstr.getvalue()

    filepath.close()
    device.close()
    retstr.close()
    return text


if __name__ == "__main__":
    string='miller2019.pdf'
    string = string.encode(encoding='UTF-8')
    text = pdf_to_text(string)
    print(text)

path_to_pdf = 'pdfSciDirect/'
pdf_files=[pos_pdf for pos_pdf in os.listdir(path_to_pdf) if pos_pdf.endswith('.pdf')]
for pdf_file in pdf_files:
    call(['pdf2txt.py','-o',pdf_file[:-3]+'txt',pdf_file])
    

from subprocess import call
import glob



textList3=[]
#idList=[]
path_to_txt = '/Users/weinie/Documents/scidownl/scidownl-master/scidownl/paper'
txt_files = [pos_txt for pos_txt in os.listdir(path_to_txt) if pos_txt.endswith('.txt')]
for file in txt_files:
    txt_file=open('/Users/weinie/Documents/scidownl/scidownl-master/scidownl/paper/'+file) #read json file stored in data folder
    textList3.append(txt_file.read())
    
    

#co-occurrence matrix

import numpy as np
import nltk
from nltk import bigrams
import itertools
import pandas as pd
 
 
def generate_co_occurrence_matrix(corpus):
    vocab = set(corpus)
    vocab = list(vocab)
    vocab_index = {word: i for i, word in enumerate(vocab)}
 
    # Create bigrams from all words in corpus
    bi_grams = list(bigrams(corpus))
 
    # Frequency distribution of bigrams ((word1, word2), num_occurrences)
    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))
 
    # Initialise co-occurrence matrix
    # co_occurrence_matrix[current][previous]
    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))
 
    # Loop through the bigrams taking the current and previous word,
    # and the number of occurrences of the bigram.
    for bigram in bigram_freq:
        current = bigram[0][1]
        previous = bigram[0][0]
        count = bigram[1]
        pos_current = vocab_index[current]
        pos_previous = vocab_index[previous]
        co_occurrence_matrix[pos_current][pos_previous] = count
    co_occurrence_matrix = np.matrix(co_occurrence_matrix)
 
    # return the matrix and the index
    return co_occurrence_matrix, vocab_index
 
 
text_data = [['Where', 'Python', 'is', 'used'],
             ['What', 'is', 'Python' 'used', 'in'],
             ['Why', 'Python', 'is', 'best'],
             ['What', 'companies', 'use', 'Python']]
text_data=ProcessList2
 
# Create one list using many lists
data = list(itertools.chain.from_iterable(text_data))
matrix, vocab_index = generate_co_occurrence_matrix(data)
 
 
data_matrix = pd.DataFrame(matrix, index=vocab_index,
                             columns=vocab_index)

i=0
for val in data_matrix["innovation-lab"]:
    if val>0:
        print(data_matrix.columns.values[i],'-> ',val)
    i=i+1

        
def findWord(word,document):
    result=False
    subs=word
    chr=document
    res=chr.find(subs)
    if res>-1:
        result=True
    return result

def findWordinList(word,List):
    result=False
    for w in List:
        if w==word:
            result=True
    return result



count=0
for i in range(len(lowerList)):
    result=False
    subs=['pilot plant','pilot-plant','pilot line','pilot-line','pilot-scale plant','pilot scale plant','demonstration plant','demonstration facility','pilot facility','pilot lab','prototype plant','prototyping plant','prototype platform','prototyping platform']
    #subs=['testbed','test-bed','test bed']
    #subs=['living lab','living labs','living laboratories','living laboratory','innovation lab','innovation laboratory','innovation laboratories','innovation center','innovation centre','innovation space','innovation hub']
    for word in subs:
        if(not result):
            result=findWord(word,lowerList[i])
    if(result):count=count+1

#print after each epoch
from gensim.models.callbacks import CallbackAny2Vec

class callback(CallbackAny2Vec):
    '''Callback to print loss after each epoch.'''

    def __init__(self):
        self.epoch = 0
        self.loss_to_be_subed = 0

    def on_epoch_end(self, model):
        loss = model.get_latest_training_loss()
        loss_now = loss - self.loss_to_be_subed
        self.loss_to_be_subed = loss
        print('Loss after epoch {}: {}'.format(self.epoch, loss_now))
        self.epoch += 1

#CBOW training
model = Word2Vec(ProcessList2, workers=20, min_count=200, iter=20, window=6, compute_loss=True, callbacks=[callback()])
model2 = Word2Vec(ProcessList2, workers=20, min_count=150, iter=30, window=5, compute_loss=True, callbacks=[callback()])
model3 = Word2Vec(ProcessList2, workers=20, min_count=50, iter=20, window=5, compute_loss=True, seed=1, callbacks=[callback()]) #better
model4 = Word2Vec(ProcessList2, workers=20, min_count=100, iter=50, window=5, compute_loss=True, seed=1, callbacks=[callback()])
model5 = Word2Vec(ProcessList2, workers=20, min_count=100, iter=80, window=5, compute_loss=True, seed=1, callbacks=[callback()])
model5 = Word2Vec(ProcessList2, workers=20, min_count=15, iter=50, window=4, compute_loss=True, seed=1, callbacks=[callback()])
model6 = Word2Vec(ProcessList2, workers=40, min_count=100, iter=20, window=5, size=150, compute_loss=True, seed=1, callbacks=[callback()])
model4_1 = Word2Vec(ProcessList2, workers=20, min_count=100, iter=30, window=5, compute_loss=True, seed=1, callbacks=[callback()])

#SKIPG training
skipg = Word2Vec(ProcessList2, sg=1, workers=20, min_count=100, iter=10, window=7, size=350, compute_loss=True, seed=1, callbacks=[callback()])
skipg = Word2Vec(ProcessList2, sg=1, workers=20, min_count=50, window=6, size=300, compute_loss=True, seed=1, callbacks=[callback()])
skipg2 = Word2Vec(ProcessList2, sg=1, workers=20, min_count=50, window=6, compute_loss=True, seed=1, callbacks=[callback()]) #best
skipg3 = Word2Vec(ProcessList2, sg=1, workers=10, min_count=50, window=7, compute_loss=True, seed=1, callbacks=[callback()]) 
skipg4 = Word2Vec(ProcessList2, sg=1, workers=10, min_count=30, window=6, compute_loss=True, seed=1, callbacks=[callback()]) #best
skipg5 = Word2Vec(ProcessList2, sg=1, workers=10, min_count=15, window=5, iter=10, compute_loss=True, seed=1, callbacks=[callback()]) 

#new dataset
skipg6 = Word2Vec(ProcessList2, sg=1, workers=10, min_count=50, window=6, compute_loss=True, seed=1, callbacks=[callback()]) 
skipg7 = Word2Vec(ProcessList2, sg=1, workers=10, min_count=50, window=6, compute_loss=True, size=250,seed=1, callbacks=[callback()]) 
skipg8 = Word2Vec(ProcessList2, sg=1, workers=10, min_count=40, window=6, compute_loss=True, seed=1, callbacks=[callback()]) 

#algorithm 1
RT=[]
seeds=['sandbox','test-bench','testbed','pilot-plant','living-lab','pilot-line','pilot-facility','pilot-scale-plant','innovation-lab','innovation-hub','prototype-plant','prototyping-platform','open-innovation','facility','tool','test','experiment','hardware','software','federation','feasibility','vulnerability','stakeholder','user','prototype','creation','collaboration','participation','viability','virtual','modular','innovation','demonstration','heterogenous','heterogeneous','fidelity']
RT=seeds
for seed in seeds:
    for x in skipg4.wv.most_similar(seed,topn=500):
        if x[1]>0.5:
            RT.append(x[0])
RT=set(RT)
#algorithm 2
from nltk.stem import WordNetLemmatizer 
from nltk.corpus import wordnet as wn
from collections import defaultdict
from nltk import pos_tag
#build dictionary
tag_map = defaultdict(lambda : wn.NOUN)
tag_map['J'] = wn.ADJ
tag_map['V'] = wn.VERB
tag_map['R'] = wn.ADV
lemmatizer = WordNetLemmatizer()    

for token, tag in pos_tag(RT):
    lemma = lemmatizer.lemmatize(token, tag_map[tag[0]])
    if tag=='VBD':
        propertyList.append(token)
    else:
        if tag == 'NNS' or tag=='NN' or tag=='JJ':
            conceptList.append(token)
            #print(token, '-->', tag)
def get_wordnet_pos(word):
    """Map POS tag to first character lemmatize() accepts"""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)
propertyList=[]
conceptList=[]
for word in RT:
    pos=get_wordnet_pos(word)
    if pos == 'n' or pos =='a':
        conceptList.append(word)
    else:
        if pos == 'v':
            propertyList.append(word)
            
#write to csv
import csv

with open('concept.csv', mode='w') as file:
    writer = csv.writer(file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
    for word in conceptList:
        writer.writerow([word])

#algorithm 3: BIRCH clustering
# get vector

vectorList=[]
for word in conceptList:
    vectorList.append(skipg4.wv.word_vec(word))
#Phase 1
from sklearn.cluster import Birch
brc = Birch(n_clusters=None)
brc.fit(vectorList)
centroids=brc.subcluster_centers_
labels=brc.subcluster_labels_
#brc.fit_predict(vectorList)
brc.subcluster_labels_
brc.predict(vectorList)
brc.subcluster_centers_ 
brc.root_
brc._CFSubcluster.centroid_


X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1],[0,-0.3],[0,0.3],[1,0.3],[1,-0.3]]
brc = Birch(branching_factor=300, n_clusters=None, threshold=2.5,compute_labels=True)
brc.fit(vectorList)
res=brc.predict(vectorList)
brc.subcluster_labels_






#clu=brc.fit_predict(vectorList)
#brc.partial_fit()
#Phase 2
#get clusters
cluster1=[]
cluster2=[]
cluster3=[]
cluster4=[]
cluster5=[]
cluster6=[]
cluster7=[]
cluster8=[]
cluster9=[]

i=0
for label in res:
    if label==0:
        cluster1.append(conceptList[i])
    elif label==1:
        cluster2.append(conceptList[i])  
    elif label==2:
        cluster3.append(conceptList[i])
    elif label==3:
        cluster4.append(conceptList[i])
    elif label==4:
        cluster5.append(conceptList[i])  
    elif label==5:
        cluster6.append(conceptList[i])
    elif label==6:
        cluster7.append(conceptList[i])
    elif label==7:
        cluster8.append(conceptList[i]) 
    elif label==8:
        cluster9.append(conceptList[i])
    #elif label==9:
        #cluster10.append(conceptList[i])
    i=i+1   

#find word



#-------Phase 2: use existing algorithm
#remove granite
i=0
for word in conceptList:
    if word=='pilot-line':
        print(i)
    i=i+1
    
#given vector find words
for vec in cluster2:
    similars = skipg4.wv.most_similar(positive=[vec])
    print(similars[0])

from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import KMeans
#for each cluster do below:
vec=[]
for word in pcluster:
    vec.append(skipg4.wv.word_vec(word))
clustering = AgglomerativeClustering(n_clusters=None,distance_threshold=3.5,linkage='average').fit(vec)
clustering.labels_
max(clustering.labels_)
for ind in range(len(clustering.labels_)):
    if clustering.labels_[ind]>1:
        print(clustering.labels_[ind],'->',cluster1[ind])

#Kmeans
kmeans = KMeans(n_clusters=5).fit(vec)
res2=kmeans.labels_

Sum_of_squared_distances = []
K = range(1,10)
for k in K:
    km = KMeans(n_clusters=k)
    km = km.fit(vec)
    Sum_of_squared_distances.append(km.inertia_)

plt.figure(figsize=(12, 7))
plt.plot(K, Sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('Sum_of_squared_distances')
plt.title('Elbow Method For Optimal k')
plt.show()

#derivative
from numpy import diff
dydx = diff(Sum_of_squared_distances)/diff(K)
d2=diff(dydx)
max_value = abs(d2[0])
max_i=0
for i in range(len(d2)):
    abs(d2[i])>max_value
    max_value=abs(d2[i])
    max_i=i
print(max_i)

#elbow 2
def calculate_WSS(points, kmax):
  sse = []
  for k in range(1, kmax+1):
    kmeans = KMeans(n_clusters = k).fit(points)
    centroids = kmeans.cluster_centers_
    pred_clusters = kmeans.predict(points)
    curr_sse = 0
    
    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS
    for i in range(len(points)):
      curr_center = centroids[pred_clusters[i]]
      curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2
      
    sse.append(curr_sse)
  return sse

calculate_WSS(vec,50)
#silhouette_score
from sklearn.metrics import silhouette_score

sil = []
kmax = 30

# dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2
for k in range(2, kmax+1):
  kmeans = KMeans(n_clusters = k).fit(vec)
  labels = kmeans.labels_
  sil.append(silhouette_score(vec, labels, metric = 'euclidean'))

k=range(2,31)
plt.plot(k, sil, 'bx-')
plt.xlabel('k')
plt.ylabel('silhouette_score')
plt.show()

#Now fit the k means
kmeans = KMeans(n_clusters = 6).fit(vec)
labels = kmeans.labels_

#print subcluster
for i in range(max(labels)):
    j=0
    for lab in labels:
        if lab==i:
            print(i,'->',conceptList[j])
        j=j+1

#count number of each cluster
for i in range(max(res2)):
    j=0
    count=0
    for lab in res2:
        if lab==i:
           # print(i,'->',conceptList[j])
           count=count+1
        j=j+1
    print(i,'-> ',count)



#algorithm 4

propArr=[]
for prop in propertyList:
    sim=0
    tempsim=0
    prop_vec=skipg4.wv.word_vec(prop)
    allocatedCon=''
    #allocatedCon=conceptList[0]
    for con in conceptList:
        con_vec=skipg4.wv.word_vec(con)
        tempsim=skipg4.wv.similarity(prop,con)
        if tempsim>sim:
            sim=tempsim
            allocatedCon=con
    propArr.append(allocatedCon)
for i in range(len(propArr)):
    print(propertyList[i],'-> ',propArr[i])
    
removeList=[] 
with open('REMOVE.csv') as csv_file:
   line_count=0
   csv_reader = csv.reader(csv_file, delimiter=',')
   for row in csv_reader:   
       removeList.append(','.join(row))

for i in range(len(removeList)):
    if i>0:
        conceptList.remove(removeList[i])
for i in range(len(removeList)):
    propertyList.append(removeList[i])
    



#plot 
#-ver2
import numpy as np

from matplotlib import pyplot as plt
from scipy.cluster.hierarchy import dendrogram
from sklearn.datasets import load_iris
from sklearn.cluster import AgglomerativeClustering

def plot_dendrogram(model, **kwargs):

    # Children of hierarchical clustering
    children = model.children_

    # Distances between each pair of children
    # Since we don't have this information, we can use a uniform one for plotting
    distance = np.arange(children.shape[0])

    # The number of observations contained in each cluster level
    no_of_observations = np.arange(2, children.shape[0]+2)

    # Create linkage matrix and then plot the dendrogram
    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)

    # Plot the corresponding dendrogram
    #R=dendrogram(linkage_matrix,**kwargs)
    R=dendrogram(linkage_matrix,show_leaf_counts=True)

vec=[]
for word in charList:
    vec.append(skipg4.wv.word_vec(word))
    
#-----original example----
iris = load_iris()
x = iris.data[:20]
model = AgglomerativeClustering(n_clusters=3)
model = model.fit(x)
#-----end---------

model = AgglomerativeClustering(n_clusters=None,distance_threshold=4.4) #cluster1:3.4; cluster2:5.2; cluster3: 4; cluster4:3.6; cluster5:5.5;cluster7:5;cluster8:4.5;cluster9:5
model=model.fit(vec)
max(model.labels_)

    



plt.title('Hierarchical Clustering Dendrogram')
plt.figure(figsize=(16, 9))
plot_dendrogram(model, labels=model.labels_)

plt.savefig('cluster9.png', format='png', dpi=150, bbox_inches='tight')
plt.show()


#get agg cluster distribution
for i in range(max(model.labels_)):
    count=0
    for lab in model.labels_:
        if lab==i:
            count=count+1
            #print(i,'-> ',cluster2[i])
    print(i,'-> ',count)
    
#print sub-clusters
for i in range(max(model.labels_)):
    j=0
    for lab in model.labels_:
        if lab==i:
            print(i,'->',cluster7[j])
        j=j+1

#count
count=0
text=[]
for doc in ProcessList2:
    count=count+len(doc)
    for word in doc:
        text.append(word)
print(count)
#write to csv
import csv

with open('download.csv', mode='w') as file:
    writer = csv.writer(file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
    for word in download:
        writer.writerow([word])